---
title: "BDA - Project"
author: "Anonymous"
output: 
  pdf_document: 
    toc: yes
    toc_depth: 3
---

```{r setup, include=FALSE}
# This chunk sets echo = TRUE as default, that is print all code.
# knitr::opts_chunk$set can be used to set other notebook generation options, too.
# include=FALSE inside curly brackets makes this block not be included in the pdf.
knitr::opts_chunk$set(echo = TRUE)
rm(list = ls()) # remove all output
library(reticulate) # MUST BE INSTALLED via remotes::install_github("rstudio/reticulate")
use_condaenv('bda')
```

# 1. Project Introduction

Road traffic and safety have become one of the major problems in people's safety concern. 
According to [WHO](https://www.who.int/publications/i/item/9789241565684), the annual road traffic deaths has reached 1.35 million in 2018, which makes road accident the leading killer of people aged from 5 to 29. 
In the UK, traffic accidents has caused more than 1700 deaths and more than 150,000 injuries in 2019 alone [source](https://www.racfoundation.org/motoring-faqs/safety#a1). 
Therefore, understanding and projecting the trend of growth (decrease) about the number of traffic accidents, could raise the awareness of the general population and call for collaborative effort to address this problem.

In this project, we try to explore the `Road Safety Data` from the Department of Transport in the UK. 
The dataset accurately presents the time, location, police force, vehicles and number of citizens involved in every accident, and it is publicly available at [Road Safety Data](https://data.gov.uk/dataset/cb7ae6f0-4be6-4935-9277-47e5ce24a11f/road-safety-data). 
We will try to capture the trend of the number of cases in different areas using a normal model with linear mean, and provide statistical results in a Bayesian perspective. 
Concretely, we study the number of accidents in 6 areas: Metropolitan Area (London), Cumbria, Lancashire, Merseyside, Greater Manchester and Cheshire. 

The remaining contents of this report are structured as follows: 
Section 2 presents the process of data pre-processing and information extraction. 
It also provides an intuitive overview with the visualization of the elementary statistics. 
Section 3 introduces and tests the probability models that we choose for this dataset, which includes a separate model, a pooled model and a hierarchical model.
Section 4 discusses the fitting results of the three models and evaluates the quality of them based on convergence, cross validation and sensitivity. 
Finally, Section 5 draws a conclusion for our project and looks into possible methods and outcome of future work.
This submission is completed in `python` with `pystan`.

```{r, engine='python'}
import numpy as np 
import matplotlib.pyplot as plt 
# with out this, plots from matplotlib won't knit on windows
import matplotlib
matplotlib.use('TkAgg') 
import pystan
import arviz as az
from pathlib import Path
from matplotlib.patches import Patch
from matplotlib.lines import Line2D

verbose=False

import pystan
print("pystan version:", pystan.__version__)

```

# 2. Data Preprocessing and Visualization

## 2.X Elementary Statistics
After preprocessing of the raw data, we could perform some elementary statistical analysis to obtain an intuitive grasp of the data.
The data here summarizes the number of accidents in 6 areas from 2005 to 2019 (15 years in total), therefore the processed data have size (6,15).
```{python}
area_names = ["Metropolitan Police", 'Cumbria','Lancashire',
               'Merseyside','Greater Manchester','Cheshire']
model_path = './Stan'
data_file = './data/data.txt'
accident_data = np.loadtxt(data_file)
print(accident_data.shape)
```

First, we could calculate the mean accident rate in each area, which turns out to be approximately $25$.
```{python}
# mean value approximately 25 cases per 10,000 people
mean_value = np.mean(accident_data) 
mean_value
```

Then, we fit a simple linear model with least mean square method to the data points.
From the results, we could see that the number of cases in the Metropolitan Police area has been increasing slowly, while the  cases in the other five police areas have been decreasing with similar trends.
```{python}
plt.figure(figsize=(12, 6));
years = np.arange(2005, 2020, 1).astype(np.int)
for i in range(6):
    plt.scatter(years, accident_data[i, :], marker='.', s=20)
    fit = np.polyfit(years, accident_data[i, :], 1)
    fitted_values = np.polyval(fit, years)
    plt.plot(years, fitted_values, label=area_names[i])
plt.legend()
plt.show()
```



# 3. Probability Models
For this dataset, we use nomal probability models with linear mean, which could be generally written as:
\begin{equation}
accidents \sim Normal(\alpha + \beta * year, \sigma)
\end{equation}
Intuitively, it is very unlikely that the accident rate change by 50% of the mean (approximately 25), so we could choose the variance of the slope $\beta$ in the linear mean.
From a standard normal distribution table, we've found that $-2.57$ corresponds to $0.5%$ in the normal CDF, therefore the interval $[-2.57\sigma, 2.57\sigma]$ contains $99%$ of the probability mass.
Based on these facts, we could use $2.57 \sigma = 0.5mean$ to find a candidate for variance of the slope, and the results is 4.85.
```{python}
# it's very un likely to change 50% of the mean, so 2.57*sigma = mean_value/2
sigma_cand = mean_value / (2*2.57) 
sigma_cand
```
Then, we build separate, pooled and hierarchical model to fit our data.

## 3.1 Separate Model
In a separate model, we treat each district as an individual entity, and assign independent parameters to them. Specifically, we assign individual parameters $\alpha_i$ and $\beta_i$ to the $i$th area, and make the mean vary linearly with respect to years. But each district will have a constant variance across all 15 years. The mathematical expression for the separate model can be specified with the following equations, where the prior choices are based on the analysis above.
$$
\begin{aligned}
\alpha_i &\sim Normal(30, 20)\\
\beta_i &\sim Normal(0, 4.85)\\
\sigma_j &\sim uniform \\
\mu_{i,j} &= \alpha_i + \beta_i * year_j \\
accident[i, j] &\sim Normal(\mu_{i,j}, \sigma_j)
\end{aligned}
$$
Both the intercept and slope are modelled with a normal prior.
From the linear least mean square fitting, we observe that the mean values are around 25, therefore we set the mean value of $\alpha$'s prior to a number nearby (30).
Besides, we've kept the variance large to provide relatively weakly informative prior, because we observed large differences among the areas.
The prior for $\beta$ centers around zero and has a variance at 4.85 as discussed above.


Here we load the stan model and display its implementation.
In our models, we set 2005 to year 1 since we don't have data before that year.
Further, we implemented three sets of priors for sensitivity analysis (Section 4.5) and prepared log likelihood for cross validation.
```{python}
separate_model_name = 'accident_separate.stan'
separate_stan_model = pystan.StanModel(file=model_path + '/' + separate_model_name)
print(separate_stan_model.model_code)
```

We define a common function to test the three different stan models, and for simplicity we just report the estimated values of the key parameters $\alpha$, $\beta$ and $\sigma$.
```{python}
def test_stan_model(stan_model, data, verbose = False):   
    data_for_stan = dict(
        N = data.shape[0],
        Y = data.shape[1],
        accidentData = data,
        years = np.arange(1, data.shape[1]+1), # stan index starts from 1
        xpred=2020,
        prior_choice=1
    )
    stan_results = stan_model.sampling(data=data_for_stan)
    if verbose:
        print(stan_results)
    else:
        print(stan_results.stansummary(pars=["alpha", "beta", "sigma"]))
        
    return stan_results
```

From the summary by stan, we could see that the accident rate in Metropolitan Area has slightly increased in these years, but all the other five areas have witnessed a decrease in accident rate.
In this part, the fitted model agrees with our intuition and elementary statistical results.
```{python}
separate_results = test_stan_model(separate_stan_model, accident_data, verbose=verbose)
```

## 3.2 Pooled Model
In the pooled model, all the 6 areas obeys the same normal distribution.
The mathematical expression of this model can be specified by the following equations:
$$
\begin{aligned}
\alpha &\sim Normal(30, 20)\\
\beta &\sim Normal(0, 4.85)\\
\sigma_j &\sim uniform \\
\mu_{j} &= \alpha + \beta * year_j \\
accident[:, j] &\sim Normal(\mu_{j}, \sigma_j)
\end{aligned}
$$
Note that for fair comparison of the models, we use the same prior as the separate model here.

Again, we prepared three sets of priors for sensitivity checking.
```{python}
pooled_model_name = 'accident_pooled.stan'
pooled_stan_model = pystan.StanModel(file=model_path + '/' + pooled_model_name)
print(pooled_stan_model.model_code)
```

The summary for the pooled model is shown below.
From the results, we could see that the value for $\alpha$ and $\beta$ is around the mean of the multiple $\alpha_i$ and $\beta_i$ above.
However, the variance is larger than the separate model, since it has to cover different scenarios in all areas.
```{python}
pooled_results = test_stan_model(pooled_stan_model, accident_data, verbose=verbose)
```


## 3.3 Hierarchical Model

```{python}
hier_model_name = 'accident_hierarchical.stan'
hier_stan_model = pystan.StanModel(file=model_path + '/' + hier_model_name)
print(hier_stan_model.model_code)
```

```{python}
hier_results = test_stan_model(hier_stan_model, accident_data, verbose=verbose)
```


# 4. Model Evaluation

In this section, we would like to evaluate three models using convergence analysis, cross-validaton with PSIS-LOO, posterior predictive checking and prior sensitivity test.
```{python}
vars2plot =["alpha", "beta", "sigma"] # the variables that need to be plotted
```

## 4.1 Convergence Analysis

As the third section described, we use linear model for prediction, so in this section, we check the distribution of alpha, beta (which determines the mean value of normal distribution) and sigma (variance of normal distribution).

### 4.1.1 Rhat value
As we can see, the $\hat{R}$ of $\alpha$ and $\beta$ are fairly close to 1.
The idea behind $\hat{R}$ is the law of total variance:
$$
  Var(X) = E[Var(X|Y)] + Var\left(E[X|Y]\right),
$$
where the first term means average variability within a chain ($W$), and the second means variability between chains ($B$).
$\hat{R}$ is total variance divided by $W$ then take square root:
$$
  \hat{R} = \sqrt{\frac{N-1}{N}+\frac{1}{N}\frac{B}{W}} = \sqrt{1 + \frac{1}{N}(\frac{B}{W}-1)}
$$
From the expression we could see, if $N$ is large enough, $\hat{R}$ will converge to 1.
If $B$ is much smaller than $W$, $\hat{R}$ could be smaller than 1, which means the chains have explored similar area in the distribution and they do not have much differences.
If $B$ is much larger than $W$, $\hat{R}$ could be somewhat larger than 1, which means the chains are fairly different from each other and the exploration of the distribution is not enough.

### 4.1.2 Effective Sample Sizes
In general, MCMC methods raises an issue that samples will be auto-correlated within a chain. 
And effective sample size(ESS) is thus used to check if a sample was simple random sample. 
To evaluate the convergence of our models, we comply with such a rule: the higher the effective sample size, the more likely the chains for the draws have converged. 
As [1](https://www.displayr.com/what-is-effective-sample-size/) stated, ESS should be at least 100 times the number of chains. 
Therefore, we plot the following figures: the dotted line represents 100*n_chains (We train 4 chains in practice). 
We can firmly conclude our model has converged since all ESS values obviously are over the dotted line.
```{python}
_ = az.plot_ess(
    separate_results, var_names=vars2plot, 
    kind="local", marker="_", ms=20, mew=2, figsize=(20, 20)
)
plt.show()
```


```{python}
_ = az.plot_ess(
    pooled_results, var_names=vars2plot, 
    kind="local", marker="_", ms=20, mew=2, figsize=(20, 5)
)
plt.show()
```


```{python}
_ = az.plot_ess(
    hier_results, var_names=vars2plot, 
    kind="local", marker="_", ms=20, mew=2, figsize=(20, 20)
)
plt.show()
```
### 4.1.3 Chains Analysis
We also plot all chains in the sampling process. 
As figures shown, all chains are fluctuated at a small range, and they are also overlapping. 
Therefore, these chains have converged and traced out a common distribution.
```{python}
_ = az.plot_trace(separate_results, var_names = vars2plot, figsize=(20, 30))
plt.show()
```


```{python}
_ = az.plot_trace(pooled_results, var_names = vars2plot, figsize=(20, 10))
plt.show()
```

```{python}
_ = az.plot_trace(hier_results, var_names = vars2plot, figsize=(20, 30))
plt.show()
```

### 4.1.4 HMC/NUTS

"The validity of the estimates is not guaranteed if there are post-warmup divergences."[2](https://mc-stan.org/misc/warnings.html)
To avoid divergent transitions, we firstly check out the maximum treedepth for draws and increase treedepth parameter in the training process. 
Secondly, we evaluate if our training process produces any divergent transitions. 
Results shown below also prove our models are convergent.

```{python}
def get_treedepth(stan_results):    
    h = stan_results.to_dataframe(diagnostics=True)
    print('max treedepth for draws: ', h['treedepth__'].max())
    print('min treedepth for draws: ', h['treedepth__'].min())
    print('mean treedepth for draws: ', h['treedepth__'].mean())
    print('treedepth quantiles:\n', h['treedepth__'].quantile([0, 0.25, 0.5, 0.75, 1]))
    print('divergent transitions: ', any(h['divergent__']))
```

```{python}
get_treedepth(separate_results)
get_treedepth(pooled_results)
get_treedepth(hier_results)
```


## 4.2 Cross-Validation with PSIS-LOO
In order to assess the predictive performance of the separate, pooled and hierarchical Guassian model, we uses leave-one-out cross-validation to evaluate.

```{python}
def get_psis_loo_result(stan_results):
    idata = az.from_pystan(stan_results, log_likelihood="log_lik")
    loo_results = az.loo(idata, pointwise=True)
    print(loo_results)
    khats = loo_results.pareto_k
    az.plot_khat(khats, xlabels=True, annotate=True, figsize=(12, 6))
    plt.show()
```

```{python}
get_psis_loo_result(separate_results)
```
```{python}
get_psis_loo_result(pooled_results)
```
```{python}
get_psis_loo_result(hier_results)
```


As the results shown above, separate model is the best model since it has the largest elpd_loo value. 
By contrast, the pooled model with the smallest elpd_loo estimate is the worst model. 
The hierarchical model has a slightly lower elpd_loo value than the one of separate model.

However, for the separate model, there are 2 out of 90 pareto k diagnostic values are very bad, which marginally increase unreliability of the separate model. 
This mainly result from some highly influential observations in Great Manchester. As for the hierarchical model,
 with a little sacrifice on elpd_loo value, its all k diagnostic values are lower than 0.7.

In this section, we can conclude the pooled model is the worst model. 
But there is a trade-off between performance and reliability for separate model and hierarchical model. 
We will further compare these two models in following sections.


## 4.3 Posterior Predictive Check
In this subsection, we will check posterior predictive distribution for 2020. 
In figures shown below, lines are drawn with different slopes and intercepts based on posterior distributions. 
Lines in different colors represents different police force areas. 
Blue, red and black points denote original data points, prediction number in 2020 and posterior samples respectively. 
At last, vertical lines indicates posterior and prediction variance.
We can visually compare three models based on these three plots. For the pooled model, posterior samples in each areas are extremely close and far away with original data points. 
In addition, the variance of prediction is large. For the separate model and hierarchical model, their performance is hard to visually distinguish. 
However, we can conclude that they are promising because their posterior samples in each area are close to original data points and the variance of prediction is small. 
To further compare these two models, we need quantitative evaluation, which will be introduced in the following section.

```{python}
def plot_posterior_draws(stan_results, accident_data, pooled=False):    
    plt.figure(figsize=(15,10))
    year_idx = np.arange(accident_data.shape[1])+1
    actual_years = year_idx + 2004

    colors = ['red', 'cyan', 'orange', 'gray', 'green', 'purple']
    for x in range(1, 7):
        for i in range(100):
            if pooled:
                y = stan_results["beta"][i] * year_idx + stan_results["alpha"][i]
            else:
                y = stan_results["beta"][:, x-1][i] * year_idx + stan_results["alpha"][:, x-1][i]
            _ = plt.plot(actual_years, y, color=colors[x-1], alpha=0.05)
        if pooled:
            break
        

    for x in range(1, 7):
        for j in reversed(range(1, 16)):
            yrep = stan_results['yrep[{},{}]'.format(x, j)]
            _ = plt.errorbar(
                x = actual_years[j-1], 
                y = np.mean(yrep),
                yerr=np.std(yrep), 
                fmt='--o', zorder=i+j,
                ecolor='black', capthick=2,
                color='black',
                alpha=0.5
            )

    for k in range(1, 7):
        ypred = stan_results['pred[{}]'.format(k)]
        _ = plt.errorbar(
            x = 2020, 
            y = np.mean(ypred), 
            yerr=np.std(ypred), 
            fmt='--o', zorder=i+j+100,
            ecolor='red', capthick=2,
            color='red',
        )


    _ = plt.scatter(np.tile(years, 6), accident_data.flatten(), zorder=j+i+100, edgecolors='black')
    # _ = plt.scatter(data_for_stan["years"], data_for_stan["accidentData"], zorder=j+i+100, edgecolors='black')
    _ = plt.title("Posterior predictive check")
    _ = plt.legend(bbox_to_anchor=(1.05, 1), loc='lower left', borderaxespad=0.)

    custom_lines = [
        Line2D([0], [0], color='red', lw=4, label='Metropolitan Police'),
        Line2D([0], [0], color='cyan', lw=4, label='Cumbria'),
        Line2D([0], [0], color='orange', lw=4, label='Lancashire'),
        Line2D([0], [0], color='gray', lw=4, label='Merseyside'),
        Line2D([0], [0], color='green', lw=4, label='Greater Manchester'),
        Line2D([0], [0], color='purple', lw=4, label='Cheshire'),
        Line2D([0], [0], marker='o', color='black', label='Original datapoint', markerfacecolor='b', markersize=15),
        Line2D([0], [0], marker='o', color='red', label='Predictions 2020', markersize=15),
        Line2D([0], [0], marker='o', color='black', label='Posterior samples', markersize=15),
    ]

    _ = plt.legend(handles=custom_lines, bbox_to_anchor=(1, 1))
    _ = plt.xticks(np.arange(2005, 2021), fontsize=13)
    _ = plt.yticks(fontsize=14)
    plt.show()
```



```{python}
plot_posterior_draws(separate_results, accident_data)
```

```{python}
az.r2_score(accident_data, separate_results["yrep"])
```

```{python}
plot_posterior_draws(pooled_results, accident_data, pooled=True)
```

```{python}
az.r2_score(accident_data, pooled_results["yrep"])
```

```{python}
plot_posterior_draws(hier_results, accident_data)
```

```{python}
az.r2_score(accident_data, hier_results["yrep"])
```

## 4.4 Prior Sensitivity Test

```{python}
data_dict = dict()
names = ["default_prior", "uniform_prior", "bigger_variance"]
for i in range(3):
    current_stan_data = dict(
        N = accident_data.shape[0],
        Y = accident_data.shape[1],
        accidentData = accident_data,
        years = np.arange(1, accident_data.shape[1]+1), # stan index starts from 1
        xpred=2020,
        prior_choice= i+1
    )
    data_dict[names[i]] = current_stan_data
```


```{python}
def get_plot_forest(stan_model, data_dict, pooled=False):
    if pooled:
        figsize = (20, 5)
    else:
        figsize = (20, 20)
    result_dict = dict()
    for key, stan_data in data_dict.items():
        print("Generating results with prior:{} {}".format(stan_data["prior_choice"], key))
        sampling_result = stan_model.sampling(data=stan_data)
        #print(sampling_result)
        result_dict[key] = sampling_result
    _ = az.plot_forest(
    list(result_dict.values()), 
    model_names=list(result_dict.keys()), var_names=["beta"], markersize=10,
    kind='ridgeplot', ridgeplot_overlap=3, ridgeplot_alpha=0.3, r_hat=True, \
        ess=True, figsize=figsize, textsize=20)
    plt.rcParams['xtick.labelsize'] = 20
    plt.rcParams['ytick.labelsize'] = 20
    plt.show()
```



```{python}
get_plot_forest(separate_stan_model, data_dict)
```

```{python}
get_plot_forest(pooled_stan_model, data_dict, pooled=True)
```

```{python}
get_plot_forest(hier_stan_model, data_dict)
```

# 5. Conclusion and Future Work

# Appendix

# References

